/*
 * Copyright 2023 The TensorFlow Authors. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *             http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.google.mediapipe.examples.facelandmarker

import android.content.Context
import android.graphics.Bitmap
import android.graphics.Matrix
import android.media.MediaMetadataRetriever
import android.net.Uri
import android.os.SystemClock
import android.util.Log
import androidx.annotation.VisibleForTesting
import androidx.camera.core.ImageProxy
import com.google.mediapipe.framework.image.BitmapImageBuilder
import com.google.mediapipe.framework.image.MPImage
import com.google.mediapipe.tasks.components.containers.NormalizedLandmark
import com.google.mediapipe.tasks.core.BaseOptions
import com.google.mediapipe.tasks.core.Delegate
import com.google.mediapipe.tasks.vision.core.RunningMode
import com.google.mediapipe.tasks.vision.facelandmarker.FaceLandmarker
import com.google.mediapipe.tasks.vision.facelandmarker.FaceLandmarkerResult
import kotlin.math.abs
import kotlin.math.asin
import kotlin.math.atan2
import kotlin.math.pow
import kotlin.math.sqrt

class FaceLandmarkerHelper(
    var minFaceDetectionConfidence: Float = DEFAULT_FACE_DETECTION_CONFIDENCE,
    var minFaceTrackingConfidence: Float = DEFAULT_FACE_TRACKING_CONFIDENCE,
    var minFacePresenceConfidence: Float = DEFAULT_FACE_PRESENCE_CONFIDENCE,
    var maxNumFaces: Int = DEFAULT_NUM_FACES,
    var currentDelegate: Int = DELEGATE_CPU,
    var runningMode: RunningMode = RunningMode.IMAGE,
    val context: Context,
    // this listener is only used when running in RunningMode.LIVE_STREAM
    val faceLandmarkerHelperListener: LandmarkerListener? = null
) {

    // For this example this needs to be a var so it can be reset on changes.
    // If the Face Landmarker will not change, a lazy val would be preferable.
    private var faceLandmarker: FaceLandmarker? = null

    // æ–°å¢ï¼šä¼˜åŒ–ç®—æ³•ç»„ä»¶
    private val temporalSmoother = TemporalSmoother()
    private val adaptiveThresholds = AdaptiveThresholds(context)
    private val enhancedEARCalculator = EnhancedEARCalculator()
    private val expressionAnalyzer = ExpressionAnalyzer() // æ–°å¢è¡¨æƒ…åˆ†æå™¨
    
    // æ–°å¢ï¼šæ€§èƒ½ä¼˜åŒ–ç»„ä»¶
    private val performanceOptimizer = PerformanceOptimizer()
    private val memoryPool = MemoryPool()
    
    // æ–°å¢ï¼šå¸§è®¡æ•°å™¨ç”¨äºæ§åˆ¶æ—¥å¿—é¢‘ç‡
    private var frameCounter = 0L
    
    // ç”¨äºç¯å¢ƒé€‚åº”çš„å˜é‡
    private var currentBrightness = 0.5f
    private var currentFaceSize = 0.5f

    // è¿™äº›ç´¢å¼•åŸºäº MediaPipe 468 Face Landmark model çš„å¸¸è§æ˜ å°„ (é’ˆå¯¹äººç‰©çš„å·¦çœ¼)
    // !! å¼ºçƒˆå»ºè®®æ ¹æ®ä½ ä½¿ç”¨çš„å…·ä½“æ¨¡å‹ç‰ˆæœ¬å’Œæ–‡æ¡£è¿›è¡Œæ ¸å¯¹å’Œè°ƒæ•´ !!
    private val LEFT_EYE_UPPER_LID_INDEX = 386 // äººç‰©å·¦çœ¼ä¸Šçœ¼ç‘ä¸­ç‚¹ (è¿‘ä¼¼)
    private val LEFT_EYE_LOWER_LID_INDEX = 374 // äººç‰©å·¦çœ¼ä¸‹çœ¼ç‘ä¸­ç‚¹ (è¿‘ä¼¼)

    // çœ¼ç›é—­åˆé˜ˆå€¼ (åŸºäºå½’ä¸€åŒ–yåæ ‡çš„å·®å€¼) - !! éœ€è¦å®éªŒè°ƒæ•´ !!
    private val EYE_CLOSED_THRESHOLD = 0.02f


    // --- æ–°å¢ EAR ç®—æ³•ç›¸å…³çš„å¸¸é‡ (äººç‰©å·¦çœ¼) ---
    // !! å¼ºçƒˆå»ºè®®æ ¹æ®ä½ ä½¿ç”¨çš„å…·ä½“æ¨¡å‹ç‰ˆæœ¬å’Œæ–‡æ¡£è¿›è¡Œæ ¸å¯¹å’Œè°ƒæ•´ !!
    private val LEFT_EYE_P1_INDEX = 362 // å¤–è§’
    private val LEFT_EYE_P2_INDEX = 385 // ä¸Šçœ¼ç‘ç‚¹1
    private val LEFT_EYE_P3_INDEX = 387 // ä¸Šçœ¼ç‘ç‚¹2
    private val LEFT_EYE_P4_INDEX = 263 // å†…è§’
    private val LEFT_EYE_P5_INDEX = 373 // ä¸‹çœ¼ç‘ç‚¹1
    private val LEFT_EYE_P6_INDEX = 380 // ä¸‹çœ¼ç‘ç‚¹2

    private val RIGHT_EYE_P1_INDEX = 133 // å¤–è§’ (é€šå¸¸æ˜¯133)
    private val RIGHT_EYE_P2_INDEX = 158 // ä¸Šçœ¼ç‘ç‚¹1 (è¿‘ä¼¼)
    private val RIGHT_EYE_P3_INDEX = 160 // ä¸Šçœ¼ç‘ç‚¹2 (è¿‘ä¼¼)
    private val RIGHT_EYE_P4_INDEX = 33  // å†…è§’ (é€šå¸¸æ˜¯33)
    private val RIGHT_EYE_P5_INDEX = 144 // ä¸‹çœ¼ç‘ç‚¹1 (è¿‘ä¼¼)
    private val RIGHT_EYE_P6_INDEX = 153 // ä¸‹çœ¼ç‘ç‚¹2 (è¿‘ä¼¼)


    // EAR é—­çœ¼é˜ˆå€¼ - !! éœ€è¦é€šè¿‡å®éªŒå’Œæ ¡å‡†æ¥ç¡®å®š !!
    // å¯¹äºå½’ä¸€åŒ–åæ ‡ï¼Œå…¸å‹çš„EARå€¼èŒƒå›´ï¼šççœ¼æ—¶çº¦ 0.25-0.35ï¼Œé—­çœ¼æ—¶ < 0.1 æˆ–æ›´ä½
    private val EAR_CLOSED_THRESHOLD = 0.11f // åˆå§‹çŒœæµ‹å€¼ï¼Œéœ€è¦è°ƒæ•´
    // --- ç»“æŸæ–°å¢å¸¸é‡ ---

    init {
        setupFaceLandmarker()
    }

    fun clearFaceLandmarker() {
        faceLandmarker?.close()
        faceLandmarker = null
        // æ¸…ç†æ€§èƒ½ä¼˜åŒ–ç»„ä»¶
        memoryPool.cleanup()
        performanceOptimizer.reset()
    }

    // Return running status of FaceLandmarkerHelper
    fun isClose(): Boolean {
        return faceLandmarker == null
    }

    // Initialize the Face landmarker using current settings on the
    // thread that is using it. CPU can be used with Landmarker
    // that are created on the main thread and used on a background thread, but
    // the GPU delegate needs to be used on the thread that initialized the
    // Landmarker
    fun setupFaceLandmarker() {
        // Set general face landmarker options
        val baseOptionBuilder = BaseOptions.builder()

        // Use the specified hardware for running the model. Default to CPU
        when (currentDelegate) {
            DELEGATE_CPU -> {
                baseOptionBuilder.setDelegate(Delegate.CPU)
            }
            DELEGATE_GPU -> {
                baseOptionBuilder.setDelegate(Delegate.GPU)
            }
        }

        baseOptionBuilder.setModelAssetPath(MP_FACE_LANDMARKER_TASK)

        // Check if runningMode is consistent with faceLandmarkerHelperListener
        when (runningMode) {
            RunningMode.LIVE_STREAM -> {
                if (faceLandmarkerHelperListener == null) {
                    throw IllegalStateException(
                        "faceLandmarkerHelperListener must be set when runningMode is LIVE_STREAM."
                    )
                }
            }
            else -> {
                // no-op
            }
        }

        try {
            val baseOptions = baseOptionBuilder.build()
            // Create an option builder with base options and specific
            // options only use for Face Landmarker.
            val optionsBuilder =
                FaceLandmarker.FaceLandmarkerOptions.builder()
                    .setBaseOptions(baseOptions)
                    .setMinFaceDetectionConfidence(minFaceDetectionConfidence)
                    .setMinTrackingConfidence(minFaceTrackingConfidence)
                    .setMinFacePresenceConfidence(minFacePresenceConfidence)
                    .setNumFaces(maxNumFaces)
                    .setOutputFaceBlendshapes(true)
                    .setOutputFacialTransformationMatrixes(true)
                    .setRunningMode(runningMode)

            // The ResultListener and ErrorListener only use for LIVE_STREAM mode.
            if (runningMode == RunningMode.LIVE_STREAM) {
                optionsBuilder
                    .setResultListener(this::returnLivestreamResult)
                    .setErrorListener(this::returnLivestreamError)
            }

            val options = optionsBuilder.build()
            faceLandmarker =
                FaceLandmarker.createFromOptions(context, options)
        } catch (e: IllegalStateException) {
            faceLandmarkerHelperListener?.onError(
                "Face Landmarker failed to initialize. See error logs for " +
                        "details"
            )
            Log.e(
                TAG, "MediaPipe failed to load the task with error: " + e
                    .message
            )
        } catch (e: RuntimeException) {
            // This occurs if the model being used does not support GPU
            faceLandmarkerHelperListener?.onError(
                "Face Landmarker failed to initialize. See error logs for " +
                        "details", GPU_ERROR
            )
            Log.e(
                TAG,
                "Face Landmarker failed to load model with error: " + e.message
            )
        }
    }

    // Convert the ImageProxy to MP Image and feed it to FacelandmakerHelper.
    fun detectLiveStream(
        imageProxy: ImageProxy,
        isFrontCamera: Boolean
    ) {
        if (runningMode != RunningMode.LIVE_STREAM) {
            throw IllegalArgumentException(
                "Attempting to call detectLiveStream" +
                        " while not using RunningMode.LIVE_STREAM"
            )
        }
        val frameTime = SystemClock.uptimeMillis()

        // Copy out RGB bits from the frame to a bitmap buffer
        val bitmapBuffer =
            Bitmap.createBitmap(
                imageProxy.width,
                imageProxy.height,
                Bitmap.Config.ARGB_8888
            )
        imageProxy.use { bitmapBuffer.copyPixelsFromBuffer(imageProxy.planes[0].buffer) }
        imageProxy.close()

        val matrix = Matrix().apply {
            // Rotate the frame received from the camera to be in the same direction as it'll be shown
            postRotate(imageProxy.imageInfo.rotationDegrees.toFloat())

            // flip image if user use front camera
            if (isFrontCamera) {
                postScale(
                    -1f,
                    1f,
                    imageProxy.width.toFloat(),
                    imageProxy.height.toFloat()
                )
            }
        }
        val rotatedBitmap = Bitmap.createBitmap(
            bitmapBuffer, 0, 0, bitmapBuffer.width, bitmapBuffer.height,
            matrix, true
        )

        // Convert the input Bitmap object to an MPImage object to run inference
        val mpImage = BitmapImageBuilder(rotatedBitmap).build()

        detectAsync(mpImage, frameTime)
    }

    // Run face face landmark using MediaPipe Face Landmarker API
    @VisibleForTesting
    fun detectAsync(mpImage: MPImage, frameTime: Long) {
        faceLandmarker?.detectAsync(mpImage, frameTime)
        // As we're using running mode LIVE_STREAM, the landmark result will
        // be returned in returnLivestreamResult function
    }

    // Accepts the URI for a video file loaded from the user's gallery and attempts to run
    // face landmarker inference on the video. This process will evaluate every
    // frame in the video and attach the results to a bundle that will be
    // returned.
    fun detectVideoFile(
        videoUri: Uri,
        inferenceIntervalMs: Long
    ): VideoResultBundle? {
        if (runningMode != RunningMode.VIDEO) {
            throw IllegalArgumentException(
                "Attempting to call detectVideoFile" +
                        " while not using RunningMode.VIDEO"
            )
        }

        // Inference time is the difference between the system time at the start and finish of the
        // process
        val startTime = SystemClock.uptimeMillis()

        var didErrorOccurred = false

        // Load frames from the video and run the face landmarker.
        val retriever = MediaMetadataRetriever()
        retriever.setDataSource(context, videoUri)
        val videoLengthMs =
            retriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_DURATION)
                ?.toLong()

        // Note: We need to read width/height from frame instead of getting the width/height
        // of the video directly because MediaRetriever returns frames that are smaller than the
        // actual dimension of the video file.
        val firstFrame = retriever.getFrameAtTime(0)
        val width = firstFrame?.width
        val height = firstFrame?.height

        // If the video is invalid, returns a null detection result
        if ((videoLengthMs == null) || (width == null) || (height == null)) return null

        // Next, we'll get one frame every frameInterval ms, then run detection on these frames.
        val resultList = mutableListOf<FaceLandmarkerResult>()
        val numberOfFrameToRead = videoLengthMs.div(inferenceIntervalMs)

        for (i in 0..numberOfFrameToRead) {
            val timestampMs = i * inferenceIntervalMs // ms

            retriever
                .getFrameAtTime(
                    timestampMs * 1000, // convert from ms to micro-s
                    MediaMetadataRetriever.OPTION_CLOSEST
                )
                ?.let { frame ->
                    // Convert the video frame to ARGB_8888 which is required by the MediaPipe
                    val argb8888Frame =
                        if (frame.config == Bitmap.Config.ARGB_8888) frame
                        else frame.copy(Bitmap.Config.ARGB_8888, false)

                    // Convert the input Bitmap object to an MPImage object to run inference
                    val mpImage = BitmapImageBuilder(argb8888Frame).build()

                    // Run face landmarker using MediaPipe Face Landmarker API
                    faceLandmarker?.detectForVideo(mpImage, timestampMs)
                        ?.let { detectionResult ->
                            resultList.add(detectionResult)
                        } ?: {
                        didErrorOccurred = true
                        faceLandmarkerHelperListener?.onError(
                            "ResultBundle could not be returned" +
                                    " in detectVideoFile"
                        )
                    }
                }
                ?: run {
                    didErrorOccurred = true
                    faceLandmarkerHelperListener?.onError(
                        "Frame at specified time could not be" +
                                " retrieved when detecting in video."
                    )
                }
        }

        retriever.release()

        val inferenceTimePerFrameMs =
            (SystemClock.uptimeMillis() - startTime).div(numberOfFrameToRead)

        return if (didErrorOccurred) {
            null
        } else {
            VideoResultBundle(resultList, inferenceTimePerFrameMs, height, width)
        }
    }

    // Accepted a Bitmap and runs face landmarker inference on it to return
    // results back to the caller
    fun detectImage(image: Bitmap): ResultBundle? {
        if (runningMode != RunningMode.IMAGE) {
            throw IllegalArgumentException(
                "Attempting to call detectImage" +
                        " while not using RunningMode.IMAGE"
            )
        }


        // Inference time is the difference between the system time at the
        // start and finish of the process
        val startTime = SystemClock.uptimeMillis()

        // Convert the input Bitmap object to an MPImage object to run inference
        val mpImage = BitmapImageBuilder(image).build()

        // Run face landmarker using MediaPipe Face Landmarker API
        faceLandmarker?.detect(mpImage)?.also { landmarkResult ->
            val inferenceTimeMs = SystemClock.uptimeMillis() - startTime
            return ResultBundle(
                landmarkResult,
                inferenceTimeMs,
                image.height,
                image.width
            )
        }

        // If faceLandmarker?.detect() returns null, this is likely an error. Returning null
        // to indicate this.
        faceLandmarkerHelperListener?.onError(
            "Face Landmarker failed to detect."
        )
        return null
    }

    private fun returnLivestreamResult(
        result: FaceLandmarkerResult, // ä½ éœ€è¦åˆ†æè¿™ä¸ª 'result' å¯¹è±¡
        input: MPImage
    ) {
        // æ€§èƒ½æ§åˆ¶ï¼šé¦–å…ˆæ£€æŸ¥æ˜¯å¦åº”è¯¥å¤„ç†å½“å‰å¸§ï¼ˆåœ¨ä»»ä½•å¤„ç†ä¹‹å‰ï¼‰
        if (!performanceOptimizer.shouldProcessFrame()) {
            return
        }
        
        if (result.faceLandmarks().size > 0) { // ç¡®ä¿æ£€æµ‹åˆ°äº†é¢éƒ¨
            val startTime = SystemClock.uptimeMillis()
            frameCounter++ // å¢åŠ å¸§è®¡æ•°å™¨
            
            val finishTimeMs = SystemClock.uptimeMillis()
            val inferenceTime = finishTimeMs - result.timestampMs()

            // åœ¨è¿™é‡Œæˆ–é€šè¿‡ listener.onResults ä¼ é€’åå¤„ç†ç»“æœ
            // --- å¼€å§‹ä¼˜åŒ–åçš„æ³¨æ„åŠ›åˆ¤æ–­é€»è¾‘ ---

            // ç¤ºä¾‹ï¼šè·å–é¢éƒ¨ç•Œæ ‡
            val faceLandmarks = result.faceLandmarks().get(0) // å‡è®¾åªå¤„ç†ç¬¬ä¸€ä¸ªæ£€æµ‹åˆ°çš„äººè„¸

            // --- å¼€å§‹ä¼˜åŒ–åçš„æ³¨æ„åŠ›åˆ¤æ–­é€»è¾‘ ---
            var headPoseYaw = 0f
            var headPosePitch = 0f
            var headPoseRoll = 0f
            var isHeadPoseConsideredAttentive = false
            
            // 1. å¤´éƒ¨å§¿æ€åˆ†æï¼ˆä¸åŸæ¥ç›¸åŒï¼‰
            if (result.facialTransformationMatrixes().isPresent
                && result.facialTransformationMatrixes().get().isNotEmpty()) {
                val matrixValues = result.facialTransformationMatrixes().get()[0]

                // å‡è®¾ matrixValues æ˜¯åˆ—ä¸»åºçš„ 4x4 çŸ©é˜µ
                // R = [ m0, m4, m8  ]
                //     [ m1, m5, m9  ]
                //     [ m2, m6, m10 ]

                val r00 = matrixValues[0]
                val r01 = matrixValues[4]
                val r02 = matrixValues[8]
                val r12 = matrixValues[9]
                val r22 = matrixValues[10]

                // è®¡ç®—æ¬§æ‹‰è§’ (è¿‘ä¼¼ï¼Œå•ä½ï¼šå¼§åº¦)
                val pitchRad = atan2(-r12, r22) // Pitch (ç»•Xè½´æ—‹è½¬ï¼Œç‚¹å¤´)
                val yawRad = asin(r02)          // Yaw (ç»•Yè½´æ—‹è½¬ï¼Œæ‘‡å¤´)
                val rollRad = atan2(-r01, r00)  // Roll (ç»•Zè½´æ—‹è½¬ï¼Œæ­ªå¤´)

                // è½¬æ¢ä¸ºè§’åº¦ï¼ˆä½¿ç”¨é¢„è®¡ç®—å¸¸æ•°æå‡æ€§èƒ½ï¼‰
                headPosePitch = pitchRad * RAD_TO_DEG
                headPoseYaw = yawRad * RAD_TO_DEG
                headPoseRoll = rollRad * RAD_TO_DEG

                // ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼åˆ¤æ–­å¤´éƒ¨å§¿æ€
                val yawThreshold = adaptiveThresholds.getAdjustedYawThreshold()
                val pitchThreshold = adaptiveThresholds.getAdjustedPitchThreshold()
                
                if (abs(headPoseYaw) <= yawThreshold && abs(headPosePitch) <= pitchThreshold) {
                    isHeadPoseConsideredAttentive = true
                }

                // ä¼˜åŒ–ï¼šæ¡ä»¶æ—¥å¿—è¾“å‡ºï¼Œå‡å°‘I/Oå¼€é”€
                if (BuildConfig.DEBUG && frameCounter % 10 == 0L) {
                    Log.d(TAG, "Head Pose: Pitch=${String.format("%.2f", headPosePitch)}, Yaw=${String.format("%.2f", headPoseYaw)}, Roll=${String.format("%.2f", headPoseRoll)}")
                    Log.d(TAG, "Adaptive Thresholds: Yaw=${String.format("%.2f", yawThreshold)}, Pitch=${String.format("%.2f", pitchThreshold)}")
                }

            } else {
                Log.d(TAG, "Facial transformation matrix not available for head pose analysis.")
                isHeadPoseConsideredAttentive = false // æ— æ³•åˆ¤æ–­æˆ–é»˜è®¤ä¸ä¸“æ³¨
            }

            // 2. ä½¿ç”¨å¢å¼ºçš„EARç®—æ³•è¿›è¡Œçœ¼ç›çŠ¶æ€åˆ†æ
            val (leftEyeEAR, rightEyeEAR, averageEAR) = enhancedEARCalculator.calculateEnhancedEAR(
                faceLandmarks, headPoseYaw, headPosePitch, headPoseRoll
            )
            
            // ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼åˆ¤æ–­çœ¼ç›çŠ¶æ€
            val earThreshold = adaptiveThresholds.getAdjustedEARThreshold()
            val areEyesOpenBasedOnEar = averageEAR >= earThreshold
            
            // ä¼˜åŒ–ï¼šæ¡ä»¶æ—¥å¿—è¾“å‡ºï¼Œå‡å°‘I/Oå¼€é”€
            if (BuildConfig.DEBUG && frameCounter % 10 == 0L) {
                Log.d(TAG, "Enhanced EAR - Left: ${String.format("%.4f", leftEyeEAR)}, Right: ${String.format("%.4f", rightEyeEAR)}, Avg: ${String.format("%.4f", averageEAR)}")
                Log.d(TAG, "Adaptive EAR Threshold: ${String.format("%.4f", earThreshold)}")
            }

            // 3. æå–æ‰€æœ‰ç›¸å…³çš„ Blendshape ç‰¹å¾ï¼ˆä¸åŸæ¥ç›¸åŒï¼‰
            var blendshapeFeatures = BlendshapeFeatures() // Initialize with defaults
            if (result.faceBlendshapes().isPresent && result.faceBlendshapes().get().isNotEmpty()) {
                val blendshapesCategories = result.faceBlendshapes().get()[0] // Get categories for the first face
                blendshapeFeatures = extractBlendshapeFeatures(blendshapesCategories)
                Log.i(TAG, "Blendshapes raw: JawOpen=${String.format("%.2f",blendshapeFeatures.jawOpen)}, BlinkL=${String.format("%.2f",blendshapeFeatures.eyeBlinkLeft)}")
            }

            // 4. æ„å»ºæ³¨æ„åŠ›ç‰¹å¾å¯¹è±¡
            val attentionFeatures = AttentionFeatures(
                headPoseYaw = headPoseYaw,
                headPosePitch = headPosePitch,
                headPoseRoll = headPoseRoll,
                leftEyeEAR = leftEyeEAR,
                rightEyeEAR = rightEyeEAR,
                averageEAR = averageEAR,
                blendshapes = blendshapeFeatures,
                isHeadPoseAttentive = isHeadPoseConsideredAttentive,
                areEyesOpen = areEyesOpenBasedOnEar,
                confidence = 0.8f, // åˆå§‹ç½®ä¿¡åº¦ï¼Œåç»­ä¼šç”±æ—¶åºå¹³æ»‘å™¨è°ƒæ•´
                timestamp = System.currentTimeMillis()
            )

            // 5. ä½¿ç”¨ä¼˜åŒ–çš„è§„åˆ™å¼•æ“åˆ†ææ³¨æ„åŠ›çŠ¶æ€
            var currentAttentionState = analyzeAttentionState(attentionFeatures)
            
            // 6. æ–°å¢ï¼šåˆ†æè¡¨æƒ…çŠ¶æ€
            val expressionResult = expressionAnalyzer.analyzeExpression(blendshapeFeatures)

            // 7. åˆ›å»ºåŸå§‹æ£€æµ‹ç»“æœ
            val rawAttentionResult = AttentionResult(
                state = currentAttentionState,
                confidence = 0.8f,
                features = attentionFeatures
            )

            // 8. åº”ç”¨æ—¶åºå¹³æ»‘
            val smoothedResult = temporalSmoother.addAndSmooth(rawAttentionResult)
            
            // 9. åˆ›å»ºç»¼åˆåˆ†æç»“æœ
            val comprehensiveResult = createComprehensiveResult(smoothedResult, expressionResult)

            // 10. å¦‚æœæ­£åœ¨æ ¡å‡†ï¼Œæ·»åŠ æ ¡å‡†æ ·æœ¬
            if (!adaptiveThresholds.isUserCalibrated() && 
                smoothedResult.state == AttentionState.ATTENTIVE && 
                smoothedResult.confidence > 0.7f) {
                adaptiveThresholds.addCalibrationSample(smoothedResult.features)
            }

            // 11. æ›´æ–°äººè„¸å¤§å°ç”¨äºè·ç¦»é€‚åº”
            updateEnvironmentParams(currentBrightness, calculateFaceSize(faceLandmarks))

            // ä¼˜åŒ–ï¼šæ¡ä»¶æ—¥å¿—è¾“å‡ºä¸»è¦çŠ¶æ€ä¿¡æ¯ï¼Œå‡å°‘I/Oå¼€é”€
            if (BuildConfig.DEBUG && frameCounter % 5 == 0L) {
                Log.i(TAG, "åŸå§‹çŠ¶æ€: $currentAttentionState")
                Log.i(TAG, "å¹³æ»‘åçŠ¶æ€: ${smoothedResult.state} (ç½®ä¿¡åº¦: ${String.format("%.2f", smoothedResult.confidence)})")
                Log.i(TAG, "è¡¨æƒ…çŠ¶æ€: ${expressionResult.primaryExpression} (å¼ºåº¦: ${String.format("%.2f", expressionResult.intensity)})")
                Log.i(TAG, "å­¦ä¹ çŠ¶æ€: ${expressionAnalyzer.getLearningStateFromExpression(expressionResult.primaryExpression)}")
                Log.i(TAG, "ç»¼åˆå‚ä¸åº¦: ${String.format("%.2f", comprehensiveResult.overallEngagement)}")
                Log.i(TAG, "çŠ¶æ€ç¨³å®šæ€§: ${temporalSmoother.getCurrentStateStability()}å¸§")
            }

            // æ€§èƒ½ä¼˜åŒ–ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°UIå’ŒçŠ¶æ€æ˜¯å¦æœ‰æ˜¾è‘—å˜åŒ–
            val shouldUpdateUI = performanceOptimizer.shouldUpdateUI() && 
                performanceOptimizer.hasSignificantChange(
                    smoothedResult.state,
                    expressionResult.primaryExpression,
                    comprehensiveResult.overallEngagement
                )

            if (shouldUpdateUI) {
                // æ„å»ºçŠ¶æ€ä¿¡æ¯å­—ç¬¦ä¸²å¹¶å‘é€åˆ°UI
                val statusText = buildOptimizedStatusText(
                    smoothedResult, 
                    expressionResult, 
                    comprehensiveResult,
                    isHeadPoseConsideredAttentive,
                    areEyesOpenBasedOnEar
                )
                faceLandmarkerHelperListener?.onStatusUpdate(statusText)
                performanceOptimizer.markUIUpdated()
            } else {
                // ä½¿ç”¨ç¼“å­˜çš„çŠ¶æ€æ–‡æœ¬
                performanceOptimizer.getCachedStatusText()?.let { cachedText ->
                    faceLandmarkerHelperListener?.onStatusUpdate(cachedText)
                }
            }

            // ä¼˜åŒ–ï¼šæ¡ä»¶è¾“å‡ºè°ƒè¯•ç»Ÿè®¡ä¿¡æ¯ï¼Œå‡å°‘I/Oå¼€é”€
            if (BuildConfig.DEBUG && frameCounter % 15 == 0L) {
                Log.d(TAG, adaptiveThresholds.getThresholdInfo())
                Log.d(TAG, enhancedEARCalculator.getEARStats())
                Log.d(TAG, expressionAnalyzer.getExpressionStats())
                val perfStats = performanceOptimizer.getPerformanceStats()
                Log.d(TAG, "Performance: AvgTime=${perfStats.averageProcessingTime}ms, " +
                          "ActualFPS=${perfStats.actualFPS}/${perfStats.targetFPS}, " +
                          "Skip=${String.format("%.1f", perfStats.frameSkipPercentage)}% " +
                          "(${perfStats.processedFrames}/${perfStats.totalFrames})")
            }

            // é€šè¿‡å›è°ƒä¼ é€’ç»¼åˆç»“æœ
            faceLandmarkerHelperListener?.onResults(
                ResultBundle(
                    result,
                    inferenceTime,
                    input.height,
                    input.width
                )
            )
            
            // ä¼ é€’ç»¼åˆåˆ†æç»“æœ
            faceLandmarkerHelperListener?.onComprehensiveResults(comprehensiveResult)
            
            // è®°å½•å¤„ç†æ—¶é—´ç”¨äºæ€§èƒ½ç›‘æ§
            performanceOptimizer.recordProcessingTime(startTime)
        } else {
            faceLandmarkerHelperListener?.onEmpty()
        }
    }

    /**
     * ä¼˜åŒ–çš„æ³¨æ„åŠ›çŠ¶æ€åˆ†æï¼ˆä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼ï¼‰
     */
    private fun analyzeAttentionState(features: AttentionFeatures): AttentionState {
        val blendshapeFeatures = features.blendshapes

        // ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼
        val yawnThreshold = adaptiveThresholds.yawnThreshold
        val lookSideThreshold = adaptiveThresholds.lookSideThreshold
        val lookDownThreshold = adaptiveThresholds.lookDownThreshold
        val lookUpThreshold = adaptiveThresholds.lookUpThreshold
        val browDownThreshold = adaptiveThresholds.browDownThreshold
        val eyeSquintThreshold = adaptiveThresholds.eyeSquintThreshold
        val blinkThreshold = adaptiveThresholds.blinkThreshold

        // ä¼˜å…ˆçº§é«˜çš„çŠ¶æ€å…ˆåˆ¤æ–­
        // a. æ‰“å“ˆæ¬ 
        if (blendshapeFeatures.jawOpen > yawnThreshold) {
            return AttentionState.YAWNING
        }
        // b. æ˜æ˜¾åˆ†å¿ƒ - çœ‹åˆ«å¤„
        else if (abs(features.headPoseYaw) > adaptiveThresholds.getAdjustedYawThreshold() ||
            blendshapeFeatures.eyeLookOutLeft > lookSideThreshold ||
            blendshapeFeatures.eyeLookOutRight > lookSideThreshold) {
            return AttentionState.DISTRACTED_LOOKING_AWAY
        }
        // c. æ˜æ˜¾åˆ†å¿ƒ - æŒç»­å‘ä¸‹çœ‹
        else if ((blendshapeFeatures.eyeLookDownLeft > lookDownThreshold || 
                 blendshapeFeatures.eyeLookDownRight > lookDownThreshold) && 
                 features.headPosePitch < -15f) {
            return AttentionState.DISTRACTED_LOOKING_AWAY
        }
        // d. å›°å€¦ (åŸºäºEARå’ŒBlendshapeçš„çœ¨çœ¼)
        else if (!features.areEyesOpen && 
                (blendshapeFeatures.eyeBlinkLeft > blinkThreshold || 
                 blendshapeFeatures.eyeBlinkRight > blinkThreshold)) {
            return AttentionState.DROWSY_FATIGUED
        }
        // e. åŸºç¡€çš„å¤´éƒ¨å§¿æ€å’Œçœ¼ç›çå¼€ä½œä¸º"ä¸“æ³¨"çš„åº•çº¿
        else if (features.isHeadPoseAttentive && features.areEyesOpen) {
            if ((blendshapeFeatures.browDownLeft > browDownThreshold || 
                 blendshapeFeatures.browDownRight > browDownThreshold) &&
                (blendshapeFeatures.eyeSquintLeft > eyeSquintThreshold || 
                 blendshapeFeatures.eyeSquintRight > eyeSquintThreshold) &&
                (blendshapeFeatures.mouthPressLeft > 0.3f || 
                 blendshapeFeatures.mouthPressRight > 0.3f)) {
                return AttentionState.THINKING_CONCENTRATING
            } else if (blendshapeFeatures.eyeLookUpLeft > lookUpThreshold || 
                      blendshapeFeatures.eyeLookUpRight > lookUpThreshold) {
                return AttentionState.DISTRACTED_LOOKING_AWAY
            } else {
                return AttentionState.ATTENTIVE
            }
        }
        // f. å›°æƒ‘
        else if (blendshapeFeatures.browDownLeft > browDownThreshold && 
                blendshapeFeatures.browInnerUp > 0.3f) {
            return AttentionState.CONFUSED
        }
        else {
            return AttentionState.UNKNOWN
        }
    }

    /**
     * è®¡ç®—äººè„¸å¤§å°ï¼ˆç”¨äºè·ç¦»é€‚åº”ï¼‰
     */
    private inline fun calculateFaceSize(landmarks: List<NormalizedLandmark>): Float {
        if (landmarks.size < 17) return DEFAULT_DISTANCE // ä½¿ç”¨é¢„å®šä¹‰å¸¸æ•°
        
        // ä½¿ç”¨äººè„¸è½®å»“è®¡ç®—å¤§å°
        val leftFace = landmarks[0]
        val rightFace = landmarks[16]
        val topFace = landmarks[10]
        val bottomFace = landmarks[152]
        
        val faceWidth = abs(rightFace.x() - leftFace.x())
        val faceHeight = abs(bottomFace.y() - topFace.y())
        
        // å½’ä¸€åŒ–äººè„¸å¤§å°ï¼ˆä¼˜åŒ–ï¼šå‡å°‘é™¤æ³•è¿ç®—ï¼‰
        return ((faceWidth + faceHeight) * 0.5f).coerceIn(0.1f, 1.0f)
    }

    // æ–°å¢ï¼šå¼€å§‹ç”¨æˆ·æ ¡å‡†
    fun startCalibration() {
        adaptiveThresholds.startCalibration()
        temporalSmoother.reset()
        enhancedEARCalculator.reset()
        expressionAnalyzer.reset() // é‡ç½®è¡¨æƒ…åˆ†æå™¨
        Log.i(TAG, "å¼€å§‹ç”¨æˆ·æ ¡å‡†è¿‡ç¨‹")
    }
    
    // æ–°å¢ï¼šå®Œæˆç”¨æˆ·æ ¡å‡†
    fun finishCalibration(): Boolean {
        val success = adaptiveThresholds.finishCalibration()
        Log.i(TAG, "ç”¨æˆ·æ ¡å‡†${if (success) "æˆåŠŸ" else "å¤±è´¥"}")
        return success
    }
    
    // æ–°å¢ï¼šè·å–æ ¡å‡†è¿›åº¦
    fun getCalibrationProgress(): Float {
        return adaptiveThresholds.getCalibrationProgress()
    }
    
    // æ–°å¢ï¼šæ˜¯å¦å·²æ ¡å‡†
    fun isUserCalibrated(): Boolean {
        return adaptiveThresholds.isUserCalibrated()
    }
    
    // æ–°å¢ï¼šé‡ç½®ä¸ºé»˜è®¤é˜ˆå€¼
    fun resetToDefaultThresholds() {
        adaptiveThresholds.resetToDefaults()
        temporalSmoother.reset()
        enhancedEARCalculator.reset()
        expressionAnalyzer.reset() // é‡ç½®è¡¨æƒ…åˆ†æå™¨
        performanceOptimizer.reset() // é‡ç½®æ€§èƒ½ä¼˜åŒ–å™¨
        memoryPool.cleanup() // æ¸…ç†å†…å­˜æ± 
        frameCounter = 0L // é‡ç½®å¸§è®¡æ•°å™¨
        Log.i(TAG, "é‡ç½®ä¸ºé»˜è®¤é˜ˆå€¼")
    }

    // æ–°å¢ï¼šæ›´æ–°ç¯å¢ƒå‚æ•°
    fun updateEnvironmentParams(brightness: Float, faceSize: Float) {
        currentBrightness = brightness
        currentFaceSize = faceSize
        adaptiveThresholds.adjustForLighting(brightness)
        adaptiveThresholds.adjustForDistance(faceSize)
    }

    // æ–°å¢ï¼šè·å–å½“å‰è¡¨æƒ…çŠ¶æ€
    fun getCurrentExpressionState(): ExpressionState? {
        val history = expressionAnalyzer.expressionHistory.getAll()
        return history.lastOrNull()?.primaryExpression
    }
    
    // æ–°å¢ï¼šè·å–è¡¨æƒ…ç»Ÿè®¡ä¿¡æ¯
    fun getExpressionStats(): String {
        return expressionAnalyzer.getExpressionStats()
    }
    
    // æ–°å¢ï¼šåˆ¤æ–­å½“å‰æ˜¯å¦ä¸ºç§¯æè¡¨æƒ…
    fun isCurrentExpressionPositive(): Boolean {
        val currentExpression = getCurrentExpressionState()
        return currentExpression?.let { expressionAnalyzer.isPositiveExpression(it) } ?: false
    }
    
    // æ–°å¢ï¼šåˆ¤æ–­å½“å‰æ˜¯å¦ä¸ºæ¶ˆæè¡¨æƒ…
    fun isCurrentExpressionNegative(): Boolean {
        val currentExpression = getCurrentExpressionState()
        return currentExpression?.let { expressionAnalyzer.isNegativeExpression(it) } ?: false
    }
    
    // æ–°å¢ï¼šè·å–å½“å‰å­¦ä¹ çŠ¶æ€æè¿°
    fun getCurrentLearningState(): String {
        val currentExpression = getCurrentExpressionState()
        return currentExpression?.let { expressionAnalyzer.getLearningStateFromExpression(it) } ?: "çŠ¶æ€æœªçŸ¥"
    }
    
    // æ–°å¢ï¼šè·å–è¡¨æƒ…å˜åŒ–è¶‹åŠ¿
    fun getExpressionTrend(): String {
        val history = expressionAnalyzer.expressionHistory.getAll()
        if (history.size < 3) return "æ•°æ®ä¸è¶³"
        
        val recentExpressions = history.takeLast(3)
        val positiveCount = recentExpressions.count { expressionAnalyzer.isPositiveExpression(it.primaryExpression) }
        val negativeCount = recentExpressions.count { expressionAnalyzer.isNegativeExpression(it.primaryExpression) }
        
        return when {
            positiveCount >= 2 -> "ç§¯æè¶‹åŠ¿"
            negativeCount >= 2 -> "æ¶ˆæè¶‹åŠ¿"
            else -> "ä¸­æ€§è¶‹åŠ¿"
        }
    }
    
    // æ–°å¢ï¼šé‡ç½®è¡¨æƒ…åˆ†æå™¨
    fun resetExpressionAnalyzer() {
        expressionAnalyzer.reset()
        Log.i(TAG, "è¡¨æƒ…åˆ†æå™¨å·²é‡ç½®")
    }

    // Return errors thrown during detection to this FaceLandmarkerHelper's
    // caller
    private fun returnLivestreamError(error: RuntimeException) {
        faceLandmarkerHelperListener?.onError(
            error.message ?: "An unknown error has occurred"
        )
    }

    /**
     * åˆ›å»ºç»¼åˆåˆ†æç»“æœ
     */
    private fun createComprehensiveResult(
        attentionResult: AttentionResult, 
        expressionResult: ExpressionResult
    ): ComprehensiveAnalysisResult {
        // è®¡ç®—ç»¼åˆå‚ä¸åº¦
        val overallEngagement = calculateOverallEngagement(attentionResult, expressionResult)
        
        return ComprehensiveAnalysisResult(
            attentionResult = attentionResult,
            expressionResult = expressionResult,
            overallEngagement = overallEngagement
        )
    }
    
    /**
     * è®¡ç®—ç»¼åˆå‚ä¸åº¦åˆ†æ•°
     */
    private fun calculateOverallEngagement(
        attentionResult: AttentionResult,
        expressionResult: ExpressionResult
    ): Float {
        // æ³¨æ„åŠ›çŠ¶æ€æƒé‡ (60%)
        val attentionWeight = 0.6f
        val attentionScore = when (attentionResult.state) {
            AttentionState.ATTENTIVE -> 1.0f
            AttentionState.THINKING_CONCENTRATING -> 0.9f
            AttentionState.CONFUSED -> 0.7f // å›°æƒ‘ä¹Ÿç®—ä¸€ç§å‚ä¸
            AttentionState.YAWNING -> 0.3f
            AttentionState.DROWSY_FATIGUED -> 0.2f
            AttentionState.DISTRACTED_LOOKING_AWAY -> 0.1f
            AttentionState.UNKNOWN -> 0.5f
        }
        
        // è¡¨æƒ…çŠ¶æ€æƒé‡ (40%)
        val expressionWeight = 0.4f
        val expressionScore = when (expressionResult.primaryExpression) {
            ExpressionState.EXCITED -> 1.0f
            ExpressionState.CONCENTRATED -> 0.95f
            ExpressionState.SMILING -> 0.8f
            ExpressionState.SURPRISED -> 0.7f
            ExpressionState.NEUTRAL -> 0.6f
            ExpressionState.CONFUSED -> 0.5f
            ExpressionState.FRUSTRATED -> 0.3f
            ExpressionState.BORED -> 0.2f
            ExpressionState.LAUGHING -> 0.4f // å¤§ç¬‘å¯èƒ½è¡¨ç¤ºåˆ†å¿ƒ
            ExpressionState.UNKNOWN -> 0.5f
        }
        
        // è€ƒè™‘è¡¨æƒ…å¼ºåº¦
        val intensityFactor = (expressionResult.intensity * 0.5f + 0.5f) // 0.5-1.0 range
        
        val engagementScore = (attentionScore * attentionWeight + 
                          expressionScore * expressionWeight * intensityFactor) *
                         attentionResult.confidence * expressionResult.confidence
        
        return engagementScore.coerceIn(0f, 1f)
    }

    /**
     * æ„å»ºUIçŠ¶æ€æ˜¾ç¤ºæ–‡æœ¬ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
     */
    private fun buildOptimizedStatusText(
        attentionResult: AttentionResult,
        expressionResult: ExpressionResult,
        comprehensiveResult: ComprehensiveAnalysisResult,
        isHeadPoseAttentive: Boolean,
        areEyesOpen: Boolean
    ): String {
        return memoryPool.withStringBuilder { sb ->
            
            // åŸºç¡€çŠ¶æ€ï¼ˆä¼˜åŒ–ï¼šå‡å°‘å­—ç¬¦ä¸²æŸ¥æ‰¾ï¼‰
            sb.append("ğŸ§  æ³¨æ„åŠ›çŠ¶æ€: ")
            val attentionText = when (attentionResult.state) {
                AttentionState.ATTENTIVE -> "âœ… ä¸“æ³¨"
                AttentionState.THINKING_CONCENTRATING -> "ğŸ¤” æ€è€ƒä¸“æ³¨"
                AttentionState.CONFUSED -> "ğŸ˜• å›°æƒ‘"
                AttentionState.DISTRACTED_LOOKING_AWAY -> "ğŸ‘€ åˆ†å¿ƒçœ‹åˆ«å¤„"
                AttentionState.DROWSY_FATIGUED -> "ğŸ˜´ å›°å€¦ç–²åŠ³"
                AttentionState.YAWNING -> "ğŸ¥± æ‰“å“ˆæ¬ "
                AttentionState.UNKNOWN -> "â“ æœªçŸ¥"
            }
            sb.append(attentionText)
            sb.append(" (").append(OptimizedFormatter.formatConfidence(attentionResult.confidence)).append(")\n")
            
            // è¡¨æƒ…çŠ¶æ€ï¼ˆä¼˜åŒ–ï¼šå‡å°‘å­—ç¬¦ä¸²æŸ¥æ‰¾ï¼‰
            sb.append("ğŸ˜Š è¡¨æƒ…çŠ¶æ€: ")
            val expressionText = when (expressionResult.primaryExpression) {
                ExpressionState.SMILING -> "ğŸ˜Š å¾®ç¬‘"
                ExpressionState.LAUGHING -> "ğŸ˜‚ å¤§ç¬‘"
                ExpressionState.SURPRISED -> "ğŸ˜² åƒæƒŠ"
                ExpressionState.CONFUSED -> "ğŸ˜• å›°æƒ‘"
                ExpressionState.CONCENTRATED -> "ğŸ¤” ä¸“æ³¨æ€è€ƒ"
                ExpressionState.BORED -> "ğŸ˜‘ æ— èŠ"
                ExpressionState.FRUSTRATED -> "ğŸ˜¤ æ²®ä¸§"
                ExpressionState.EXCITED -> "ğŸ¤© å…´å¥‹"
                ExpressionState.NEUTRAL -> "ğŸ˜ ä¸­æ€§"
                ExpressionState.UNKNOWN -> "â“ æœªçŸ¥"
            }
            sb.append(expressionText)
            sb.append(" (å¼ºåº¦: ").append(OptimizedFormatter.formatPercentage(expressionResult.intensity)).append(")\n")
            
            // å­¦ä¹ çŠ¶æ€ï¼ˆä¼˜åŒ–ï¼šç¼“å­˜å‡½æ•°è°ƒç”¨ç»“æœï¼‰
            val learningState = expressionAnalyzer.getLearningStateFromExpression(expressionResult.primaryExpression)
            sb.append("ğŸ“š å­¦ä¹ çŠ¶æ€: ").append(learningState).append("\n")
            
            // ç»¼åˆå‚ä¸åº¦ï¼ˆä¼˜åŒ–ï¼šé¢„è®¡ç®—å­—ç¬¦ä¸²ï¼‰
            val engagementLevel = when {
                comprehensiveResult.overallEngagement >= 0.8f -> "ğŸ”¥ éå¸¸é«˜"
                comprehensiveResult.overallEngagement >= 0.6f -> "ğŸ‘ è¾ƒé«˜"
                comprehensiveResult.overallEngagement >= 0.4f -> "ğŸ“Š ä¸­ç­‰"
                comprehensiveResult.overallEngagement >= 0.2f -> "ğŸ“‰ è¾ƒä½"
                else -> "âš ï¸ å¾ˆä½"
            }
            sb.append("ğŸ“ˆ ç»¼åˆå‚ä¸åº¦: ").append(engagementLevel)
                .append(" (").append(OptimizedFormatter.formatPercentage(comprehensiveResult.overallEngagement)).append(")\n")
            
            // ç»†èŠ‚çŠ¶æ€ï¼ˆä¼˜åŒ–ï¼šå‡å°‘æ¡ä»¶åˆ¤æ–­ï¼‰
            sb.append("\nğŸ“Š æ£€æµ‹ç»†èŠ‚:\n")
            sb.append("   â€¢ å¤´éƒ¨å§¿æ€: ").append(if (isHeadPoseAttentive) "âœ… ä¸“æ³¨" else "âŒ ä¸ä¸“æ³¨").append("\n")
            sb.append("   â€¢ çœ¼ç›çŠ¶æ€: ").append(if (areEyesOpen) "ğŸ‘ï¸ çå¼€" else "ğŸ‘ï¸â€ğŸ—¨ï¸ é—­åˆ").append("\n")
            sb.append("   â€¢ çŠ¶æ€ç¨³å®šæ€§: ").append(temporalSmoother.getCurrentStateStability()).append("å¸§\n")
            
            // è¡¨æƒ…è¶‹åŠ¿ï¼ˆä¼˜åŒ–ï¼šç¼“å­˜è°ƒç”¨ç»“æœï¼‰
            val expressionTrend = getExpressionTrend()
            sb.append("   â€¢ è¡¨æƒ…è¶‹åŠ¿: ").append(expressionTrend)
            
            val statusText = sb.toString()
            
            // ç¼“å­˜çŠ¶æ€æ–‡æœ¬
            performanceOptimizer.cacheStatusText(
                statusText,
                attentionResult.state,
                expressionResult.primaryExpression,
                comprehensiveResult.overallEngagement
            )
            
            statusText
        }
    }

    // æ–°å¢ï¼šè·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
    fun getPerformanceStats(): PerformanceStats {
        return performanceOptimizer.getPerformanceStats()
    }
    
    // æ–°å¢ï¼šè·å–å†…å­˜æ± ç»Ÿè®¡ä¿¡æ¯
    fun getMemoryStats(): MemoryPoolStats {
        return memoryPool.getStats()
    }

    companion object {
        const val TAG = "FaceLandmarkerHelper"
        private const val MP_FACE_LANDMARKER_TASK = "face_landmarker.task"

        // æ€§èƒ½ä¼˜åŒ–ï¼šé¢„è®¡ç®—çš„æ•°å­¦å¸¸æ•°
        private const val RAD_TO_DEG = 57.2958f // 180/Ï€ï¼Œé¿å…é‡å¤è®¡ç®—
        private const val DEFAULT_DISTANCE = 0.5f
        
        // æ€§èƒ½ä¼˜åŒ–ï¼šBlendshapeç´¢å¼•æ˜ å°„ï¼Œé¿å…å­—ç¬¦ä¸²æŸ¥æ‰¾
        private val BLENDSHAPE_INDICES = mapOf(
            "eyeBlinkLeft" to 0, "eyeBlinkRight" to 1,
            "eyeLookDownLeft" to 2, "eyeLookDownRight" to 3,
            "eyeLookUpLeft" to 4, "eyeLookUpRight" to 5,
            "eyeLookOutLeft" to 6, "eyeLookOutRight" to 7,
            "eyeSquintLeft" to 8, "eyeSquintRight" to 9,
            "eyeWideLeft" to 10, "eyeWideRight" to 11,
            "browDownLeft" to 12, "browDownRight" to 13,
            "browInnerUp" to 14, "browOuterUpLeft" to 15, "browOuterUpRight" to 16,
            "jawOpen" to 17,
            "mouthSmileLeft" to 18, "mouthSmileRight" to 19,
            "mouthPressLeft" to 20, "mouthPressRight" to 21,
            "mouthPucker" to 22, "mouthShrugLower" to 23, "mouthShrugUpper" to 24,
            "cheekSquintLeft" to 25, "cheekSquintRight" to 26,
            "mouthFrownLeft" to 27, "mouthFrownRight" to 28,
            "mouthRollLower" to 29, "mouthRollUpper" to 30,
            "noseSneerLeft" to 31, "noseSneerRight" to 32
        )
        
        const val DELEGATE_CPU = 0
        const val DELEGATE_GPU = 1
        const val DEFAULT_FACE_DETECTION_CONFIDENCE = 0.5F
        const val DEFAULT_FACE_TRACKING_CONFIDENCE = 0.5F
        const val DEFAULT_FACE_PRESENCE_CONFIDENCE = 0.5F
        const val DEFAULT_NUM_FACES = 1
        const val OTHER_ERROR = 0
        const val GPU_ERROR = 1
    }

    data class ResultBundle(
        val result: FaceLandmarkerResult,
        val inferenceTime: Long,
        val inputImageHeight: Int,
        val inputImageWidth: Int,
    )

    data class VideoResultBundle(
        val results: List<FaceLandmarkerResult>,
        val inferenceTime: Long,
        val inputImageHeight: Int,
        val inputImageWidth: Int,
    )

    interface LandmarkerListener {
        fun onError(error: String, errorCode: Int = OTHER_ERROR)
        fun onResults(resultBundle: ResultBundle)
        fun onEmpty() {}
        
        // æ–°å¢ï¼šçŠ¶æ€æ›´æ–°å›è°ƒ
        fun onStatusUpdate(statusText: String) {}
        
        // æ–°å¢ï¼šç»¼åˆåˆ†æç»“æœå›è°ƒ
        fun onComprehensiveResults(result: ComprehensiveAnalysisResult) {}
    }

    /**
     * ä¼˜åŒ–çš„Blendshapeç‰¹å¾æå–ï¼ˆé¿å…å­—ç¬¦ä¸²æŸ¥æ‰¾ï¼‰
     */
    private fun extractBlendshapeFeatures(blendshapesCategories: List<com.google.mediapipe.tasks.components.containers.Category>): BlendshapeFeatures {
        // æ€§èƒ½ä¼˜åŒ–ï¼šé¢„æ„å»ºæ•°ç»„é¿å…é‡å¤MapæŸ¥æ‰¾
        val values = FloatArray(33) { 0f }
        
        // ä¸€æ¬¡éå†å¡«å……æ‰€æœ‰éœ€è¦çš„å€¼
        for (category in blendshapesCategories) {
            val index = BLENDSHAPE_INDICES[category.categoryName()]
            if (index != null) {
                values[index] = category.score()
            }
        }
        
        return BlendshapeFeatures(
            eyeBlinkLeft = values[0],
            eyeBlinkRight = values[1],
            eyeLookDownLeft = values[2],
            eyeLookDownRight = values[3],
            eyeLookUpLeft = values[4],
            eyeLookUpRight = values[5],
            eyeLookOutLeft = values[6],
            eyeLookOutRight = values[7],
            eyeSquintLeft = values[8],
            eyeSquintRight = values[9],
            eyeWideLeft = values[10],
            eyeWideRight = values[11],
            browDownLeft = values[12],
            browDownRight = values[13],
            browInnerUp = values[14],
            browOuterUpLeft = values[15],
            browOuterUpRight = values[16],
            jawOpen = values[17],
            mouthSmileLeft = values[18],
            mouthSmileRight = values[19],
            mouthPressLeft = values[20],
            mouthPressRight = values[21],
            mouthPucker = values[22],
            mouthShrugLower = values[23],
            mouthShrugUpper = values[24],
            cheekSquintLeft = values[25],
            cheekSquintRight = values[26],
            mouthFrownLeft = values[27],
            mouthFrownRight = values[28],
            mouthRollLower = values[29],
            mouthRollUpper = values[30],
            noseSneerLeft = values[31],
            noseSneerRight = values[32]
        )
    }
}
